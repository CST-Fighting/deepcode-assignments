

### 了解线性方程组的求解过程、矩阵的运算（+-*）、求逆、矩阵的秩，协方差、特征值与特征向量。

#### 行列式：

$$
{\left[ {\begin{array}{*{20}{c}}
a1&a2&a3\\
b1&b2&b3
\\c1&c2&c3
\end{array}} \right]}=a1{\left[ {\begin{array}{*{20}{c}}
b2&b3\\
c2&c3
\end{array}} \right]}-a2{\left[ {\begin{array}{*{20}{c}}
b1&b3\\
c1&c3
\end{array}} \right]}+a3{\left[ {\begin{array}{*{20}{c}}
b1&b2\\
c1&c2
\end{array}} \right]}
$$

性质：

1）将一个方阵的某一行同乘数k，其余位置的元素不变，所得的新行列的值为原行列式子的k倍，
$$
{\left[ {\begin{array}{*{20}{c}}
a1&a2&a3\\
Kb1&Kb2&Kb3\\
a1&c2&c3
\end{array}} \right]}=K{\left[ {\begin{array}{*{20}{c}}
a1&a2&a3\\
b1&b2&b3\\
a1&c2&c3
\end{array}} \right]}
$$
推论：某一行全为0的方针的行列式等于0

2）互换一个方阵的两行，所得的新行列式的值为原行列式的相反数

3）将一个方阵的某一行的k倍加到另一行上，所得新方阵的行列式不变

推论：若方阵有两行相同，则行列式等于0

4）行列式可以按任意行/列展开

5）|A^T| = |A|

6)交换行列式的两行/列，行列式改变符号







#### 线性方程组的求解过程

##### 1.高斯消元法

消元法：

1）交换两个方程的上下位置

2）用一个非0的常数乘某个方程

3）把某个方程的倍数加到了另一个方程上

用矩阵演示消元的过程：

1）先将方程写成增广矩阵的形式

2）对矩阵进行的呢工行变换：将某行同×或者同除一个非零实数；将某行加入到另一行；将任意两行互换

3）将增广矩阵变换成上三角矩阵，即主对角线全为1，左下三角矩阵全为0

##### 2.回代



#### 矩阵的运算

加法：
$$
{\left[ {\begin{array}{*{20}{c}}
A&D\\
B&S
\end{array}} \right]}+{\left[ {\begin{array}{*{20}{c}}
E&R\\
W&Q
\end{array}} \right]} = {\left[ {\begin{array}{*{20}{c}}
A+E&D+R\\
B+W&S+Q
\end{array}} \right]}
$$
减法与加法类似

数乘：
$$
K{\left[ {\begin{array}{*{20}{c}}
A&D\\
D&S
\end{array}} \right]}={\left[ {\begin{array}{*{20}{c}}
KA&KD\\
DK&KS
\end{array}} \right]}
$$
提供因子：矩阵所有元素均有公因子，公因子外提一次

行列式提：一行提一次，所有元素均有公因子提n次



乘法：
$$
{\left[ {\begin{array}{*{20}{c}}
2&1&0\\
1&0&1
\end{array}} \right]}*{\left[ {\begin{array}{*{20}{c}}
1&0&1\\
0&1&1
\\0&1&1
\end{array}} \right]} = {\left[ {\begin{array}{*{20}{c}}
2&1&3\\
1&1&2
\end{array}} \right]}
$$
**note**：结果的矩阵中的第一行第一列2，为（2×1+1×0+0×0），第一个矩阵的第一行分别对应×第二矩阵的第一列，以此类推。

矩阵相乘的前提：第一个矩阵的列数=第二矩阵的行数

结果矩阵的形状：结果行数=第一个的行数，结果列数=第二个的列数

**A**mn**B**ns =**C**ms

乘法不满足：

1)AB != BA （AB有意义时，BA不一定有意义。）

2）AB = 0不能推出A !=0,或B!=0

3)AB=AC,A!=0，不能推出B=C

与零矩阵、单位矩阵E相乘



#### 逆矩阵：

若AB=BA=E,则A,B为互逆矩阵。

逆矩阵唯一

性质：

1）A可逆，不能推出|A|=0

2)((A)^(-1))^(-1 ) = A , AA* = |A|E ,|A| !=0

特殊：
$$
{\left[ {\begin{array}{*{20}{c}}
K&\\
&K
\\& &K
\end{array}} \right]^-1}={\left[ {\begin{array}{*{20}{c}}
1/K&\\
&1/K
\\&&1/K
\end{array}} \right]}
$$

$$

$$
#### 矩阵的秩：

k阶子式：在m×n矩阵A中，任取k行与k列（k<=m,k<=n），位于这些行与列的交叉点上的k2个元素按其在原来矩阵A中的次序可构成一个k阶行列式，称其为矩阵A的一个k阶子式。
设A为m×n矩阵，若A中存在r阶子式不等于0，r阶以上子式均等于0，则称矩阵A的秩为r，记为r(A).零矩阵的秩规定为0.
性质：

```
r(A)=0 <=> A=O
A≠O <=>r(A)>=1
A是n阶矩阵，r(A)=n <=>|A|≠0 <=>A可逆，r(A)<n <=>|A|≠0 <=>A不可逆
若A是m×n矩阵，则r(A)<=min(m,n)
经过初等变换矩阵的秩不变。
设A是m×n矩阵，将A以行及列分块，得则有r(A)=A的行秩=A的列值
```

公式：

```
r(A)=r(AT)；r(AAT)=r(A)
当k≠0时，r(kA)=r(A)；r(A+B)<=r(A)+r(B)
r(AB)<=min(r(A),r(B)),max(r(A),r(B))<=r(A,B)<=r(A)+r(B)
若A可逆，则r(AB)=r(B),r(BA)=r(B)
若A时m×n矩阵，B是n×s矩阵，AB=O,r(A)+r(B)<=n
```

#### 特征值与特征向量:

定义：Ann矩阵，x为非零常数，若存在数k使得Ax=kx,有非平凡的解x，则称k为特征值，x称为对应于k的特征向量。

例子：
$$
A={\left[ {\begin{array}{*{20}{c}}
1&6\\
5&2
\end{array}} \right]}    ，u={\left[ {\begin{array}{*{20}{c}}
6\\-5
\end{array}} \right]} \\
则，Au={\left[ {\begin{array}{*{20}{c}}
1&6\\
5&2
\end{array}} \right]}  *{\left[ {\begin{array}{*{20}{c}}
6\\-5
\end{array}} \right]} ={\left[ {\begin{array}{*{20}{c}}
-24\\20
\end{array}} \right]} =-4{\left[ {\begin{array}{*{20}{c}}
6\\-5
\end{array}} \right]} =-4u

\\
u对特征向量，-4为特征值
$$


#### 特征方程

由于A的特征向量是满足(A-uI)x=0的所有非平凡解，所以要使(A-uI)x=0有非平凡解，则A-uI为可逆矩阵，则det(A-uI)=0。所以称det(A-uI)=0为A的特征方程。

换句话说：数u是n*n矩阵A的特征值的充要条件是是特征方程det(A-uI)=0的根。



#### 协方差：

度量各个维度偏离其均值的程度，定义：
$$
Cov(X,Y)=E(X-E(X))(Y-E(Y))
$$
当X,Y是同一个随机变量时，X与其自身的协方差就是X的方差是协方差的一个特例：
$$
Var(X)=Cov(X,X)=E(X-E(X)^2)
$$
协方差的数值越大，两个变量同向（正相关）程度就越大。

想要比较X与Y的线性相关程度强，还是X与Z的线性相关程度强，通过cov(X,Y)与cov(X,Z)

无法直接比较。定义相关系数：
$$
N=\frac{cov(X,Y)}{\sqrt{(var(X)var(Y))}}
$$
通过X的方差var(X)var⁡(X)与Y的方差var(Y)var⁡(Y)对协方差cov(X,Y)cov⁡(X,Y)归一化，得到相关系数η，η的取值范围是[−1,1][−1,1]。1表示完全线性相关，−1表示完全线性负相关，0表示线性无关。线性无关并不代表完全无关，更不代表相互独立。

特征：协方差为0，称X和Y不相关。

性质：
$$
D(AX)=AD(X)A'\\COV(AX,BY)=ACOV(X,Y)B'=A\quad \sum_{}^{}A'
$$
协方差阵：
$$
COV(X,X)=E(X-E(X))(X-E(X))'=D(X)
$$




### 了解常见的距离：欧式距离、闵氏距离、余弦值相似度。

#### 欧氏距离：

是一个通常采用的距离定义，指在N维空间中两个点之间的真实距离，或者向量的自然长度（即该点到原点的距离）。在二维和三维空间中的欧氏距离就是两点之间的实际距离。 

公式：
$$
d(x,y):=\sqrt{(x_1-y_1)^2+....+(x_n-y_n)^2}=\quad \sum_{i=1}^{n}{(X_i - Y_i)^2}
$$

标准化欧氏距离：

解决问题：数据的各个维度之间尺度不一致。

概念： 标准化欧氏距离是针对欧氏距离的缺点而作的一种改进。标准欧氏距离的思路：既然数据各维分量的分布不一样，那先将各个分量都“标准化”到均值、方差相等，使得各个维度的数据分别满足标准正态分布。假设样本集X的均值(mean)为m，标准差(standard deviation)为s，X的“标准化变量”表示为：

```
X* = (X-m)/s
```

标准化欧氏距离公式：
$$
D12 = \sqrt{\quad \sum_{i=1}^{n}{(X_1k -X_2k)^2}}
$$


如果将方差的倒数看成一个权重，也可称之为加权欧氏距离(Weighted Euclidean distance)。

【对于尺度无关的解释】如果向量中第一维元素的数量级是100，第二维的数量级是10，比如v1=(100,10),v2 = (500,40)，则计算欧式距离：

  d = sqrt(100.(5 - 1)^2 + 10 * (4 - 1)^2)                                        

可见欧式距离会给与第一维度100权重，这会压制第二维度的影响力。



#### 闵氏距离（Minkowski Distance）

闵氏距离又叫做闵可夫斯基距离，是欧氏空间中的一种测度，被看做是欧氏距离的一种推广，欧氏距离是闵可夫斯基距离的一种特殊情况。定义式：
$$
L_p(x_i,x_j)=(\quad \sum_{i=1}^{n}|(X_i^l - Y_i^l)|^p)^{1/p}
$$
闵可夫斯基距离公式中:

当p = 2，即为欧氏距离：勾股定理计算的点之间的直接距离
$$
L_p(x_i,x_j)=(\quad \sum_{i=1}^{n}|(X_i^l - Y_i^l)|^2)^{1/2}
$$
当时p = 1，即为曼哈顿距离：点在标准坐标系上的绝对轴距之和
$$
L_p(x_i,x_j)=(\quad \sum_{i=1}^{n}|(X_i^l - Y_i^l)|)
$$
​         

#### 余弦值相似度:

几何中，夹角余弦可用来衡量两个向量方向的差异；机器学习中，借用这一概念来衡量样本向量之间的差异。

1)二维空间中向量A(x1,y1)与向量B(x2,y2)的夹角余弦公式：
$$
COS a=\frac{(x_1x_2+y_1y_2)}{\sqrt{(x_1^2+x_2^2)}\sqrt{(y_1^2+y_2^2)}}
$$
两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)的夹角余弦为：
           
$$
COSx=\vec{a} \cdot \vec{b}/|a||b|
$$
​                                                  

夹角余弦取值范围为[-1,1]。余弦越大表示两个向量的夹角越小，余弦越小表示两向量的夹角越大。当两个向量的方向重合时余弦取最大值1，当两个向量的方向完全相反余弦取最小值-1。                     



### 了解中心化 ：Min-Max标准化、Box-Cox转换。

补充：

数据的标准化:

将数据按比例缩放，使之落入一个小的特定区间，一般目的在于：去除数据的单位限制，转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权。数据的归一化便是一个典型的案例。

数据的归一化：

把数转换为(0,1)之间的小数
把有量纲的表达式转换为无量纲的表达式
归一化的好处:在多指标评价体系中，由于个评价指标的性质，通常具有不同的量纲和数量级。当各指标间的水平相差很大时，如果直接用原始指标值进行分析，就会突出数值较高的指标在综合分析中的作用，相对削弱值水平低指标的作用，因此，为了保证结果的可靠性，需要对原始数据进行标准化处理。

经验上来说，归一化就是让不同维度之间的特征在数值上有一定比较性，可以大大提高分类器的准确性。

1）数据标准化：

#### Min-Max标准化：

即max-min最大最小归一化：最大特点是能把数据规整到0-1之间，但是最大最小值容易受到异常值影响。
$$
Y_i=\frac{x_i-min(x_i)}{max(x_j)-min(x_j)}
$$
Min-Max标准化是指对原始数据进行线性变换，将值映射到[0 , 1]之间

Z-Score标准化：

基于原始数据的均值(mean)和标准差(standard deviation)来进行数据的标准化。其主要目的是将不同量级的数据统一化为同一个量级，统一用计算出的Z-Score值衡量，保证了数据间具有可比性。
$$
y_i=\frac{x_i-\overline x}{s}    \\
\overline x为均值，s为\sqrt{方差}.
$$
小数定标标准化：

移动小数点的位置来进行数据的标准化。小数点的移动的位数取决于原始数据中的最大绝对值，公式：
$$
X'=\frac{x}{10^j}\\
x为原始数据中的一个数据，x'为经过小数定标标准化后的数据，j表示满足条件的最小整数
$$




均值归一法：
$$
X= \frac{value-u}{max-min}\\
  u为均值，value为在一组数据中第i元素的值
$$


指数转换：

lg函数、Softmax函数、Sigmoid函数



#### Box-Cox转换

消除方差的方法：加权最小二乘法、方差稳定性变换、Box-Cox变换法

box-cox转换，利用极大似然估计特征的偏态情况，并利用公式对其转换成近似的正太分布。如果左偏就用左上角的log进行转换，如果右偏态就会进行正太转换。是变换族，涵盖对数/平方根(k=1/2)/倒数变换（k=-1）。

公式：
$$
y^{k}=\frac{y^k-1}{k},k!=0
y^{k}=log(y),k=0
$$
缺点：要求Y>0

推广：
$$
y^{k}=\frac{(y+a)^k-1}{k},k!=0
y^{k}=log(y+a),k=0
$$
扩展：

中心极限定理：

二项式分布：

函数概率公式：
$$
P(x) = C^x_np^x(1-p)^{n-x}
$$
当p=0.5时，分布对称，近似对称分布

当p!=0.5,分布呈偏态，特别是n较小时，p偏离0.5越远，分布的对称性越差，但只要不接近1或0时，随着n的增大，分不布逐渐逼近正态。

泊松分布：

发生概率特别小的情形。
$$
P(x)=\frac{e^{-p}p^x}{X!}
$$
偏度：随机变量X的偏度r1为三阶标准矩，定义：
$$
y_1=E\frac{k_3}{k_2^{3/2}}\\
k_3为三阶样本中心距，k_2为二阶样本中心距，即样本方差
$$
皮尔逊偏度系数：
$$
(第一)偏度=\frac{众数-均值}{标准差}
$$

$$
(第二)偏度=\frac{3(均值-中位数)}{标准差}
$$



### 了解统计知识：t检验、f检验、正态检验、卡方检验

以“总体间没差别”计算显著性水平H0（概率值）一般取0.05，拒绝原假设H0的最小显著性水平称为检验的p值。

例如：显著性水平为0.05，可以理解为只有5%的概率会出现这种显著性差异。

#### t检验:

(思想：中心极限定理)

为了比较数据样本之间是否具有显著的差异，或者是否能从样本推论到整体。

假设前提：样本服从或者近似服从正态分布，是一种参数检验方法。

验证方法：

1）独立样本的t检验：用于研究定量数据和定类数据的差异关系。（两个独立的样本，例如：学生升学与成绩）

2）单一样本的t检验：用于检验某单一的定量数据差异。（例如：一个班的成绩是否显著大于80分）

3）配对t检验：检验配对数据 的差异性。（例如：一个班上的男、女生的成绩是否有显著差异，其中女生和女生成绩成对，男生和男生成绩成对）

t检验主要通过对样本的均值的差异进行检验。
$$
t_{value}=\frac{样本均值-标准值}{样本均值的标准差}=\frac{\overline x-u}{S/\sqrt{(n)}}
$$

#### f检验:（联合假设）

如果计算出的p值大于假设显著水平H0，拒绝原假设；若小于，接收原假设。

也称方差比率检验、方差齐性检验。它是一种在零假设之下，统计值服从F-分布的检验。通常是用来分析用了超过一个参数的统计模型，以判断该模型中的全部或一部分参数是否适合用来估计母体。
其计算过程如下：
$$
F=\frac{S^2}{S^{2'}}   ---->F=\frac{组间平均差异}{组内平均差异}\\
组件平均差异=s^2=\frac{(\quad \sum_{i=1}^{n}(x-\overline x)^2)}{n-1}\\
(如果是三个及三个以上的总体均值的，则x为一个的均值，\overline x是总体的均值)
$$
再将计算得到的F值与对应的F分布表查询，若大于表中的值，则原假设成立（两样本的方差相同），否则拒绝原假设。

F的求法：

1）把n组数据放在一起，看成一个总体，算出这个总体的均值u

2)算出每组数据的组内平均值，u1，u2....

3）算出组间差异

4）算出组内差异

5）F=组间平均差异/组内平均差异





#### 正态检验:

统计假设检验的基本步骤（双边检验）：

1）做假设：H0“：u=u0（零假设）

2）选择检验假设H0的统计量，并确定其分布
$$
T=\frac{\overline X-u_0}{S/\sqrt{n}}
$$
3）据样本观测计算出该统计量的值T

4）在给定的显著性水平a(0<a<1)下，查所选的统计量服从的分布表，求出临界值t_a(n-1)。若t<=t_a(n-1)，拒绝域在左边，拒绝H0，认为u<u0;若t>t_a(n-1)，拒绝域在右边，拒绝H0，认为u>u0;









#### 卡方检验:

如果计算出的值小于假设显著水平H0，接收原假设；若大于，拒绝原假设。

卡方检验用于定类/非参数数据假设验证，适用于布尔型/二项分布数据，基于两个概率间的比较。

定义：若k个随机变量相互独立，且数学期望为0，方差为1（即服从标准正太分布），则随机变量X
$$
X=\quad \sum_{i=1}^{k}Z_n^2\\
记作：X-x^2(k)
$$
核心思想：假设H0成立，基于此前提计算出卡方分布的值，他表示观察值与理论值之间的偏离程度。根据χ2分布及自由度可以确定在H0假设成立的情况下获得当前统计量（均值/样本方差/标准差/原点距/中心距）及更极端情况的概率P。如果当前统计量大于P值，说明观察值与理论值偏离程度太大，应当拒绝无效假设，表示比较资料之间有显著差异；否则就不能拒绝无效假设，尚不能认为样本所代表的实际情况和理论假设有差别。他的无效假设H0是：观察频数与期望频数没有差别。

卡方验证的基本公式：
$$
卡方值x^2=\frac {(\quad \sum_{i=1}^{k}(A_i-E_i)^2)}{E}=\frac {(\quad \sum_{i=1}^{k}(A_i-np_i)^2)}{np_i}\\
A为观察值，E为理论期望值，K为观察值的个数。最后一个式子是具体计算的方法，n为总频数，p为理论频率，n*p是理论频数（理论值）。
$$
卡方的自由度：
$$
Df=(a-1)*(b-1)\\a和b分别为检验条件的分类数，例如：销量分为两种：高和低；成列方式有3种，所以a=2,b=3
$$
将这个结果与卡方界值表中进行比较（找自由度与显著水平0.05的香蕉的临界值），再将这个值与算出来的卡方值比较。大于，接收原假设；小于，拒绝。

卡方检验的条件：1）理论数<5的格子不能超过20%；不能有小于1的理论数

解决方案：增加样本量；列和某些分类来实现 ，可以采用2×2列联表。

### 了解最小二乘法

与同梯度下降类似，可求最优化。

一元模型：
$$
Q=min\quad \sum_{i=1}^{n}(y_ie-y_i)^2
\\y_ie是根据y=ax+b求出的估值，y_i是实际值
$$


多元线性模型：
$$
假如有变量x_1,x_2....x_n,可以用线性函数表示：
y=k_0+k_1x_1+k_2x_2+.....+k_nx_n\\
而对于m个样本来说，可用线性方程组来表示：
k_0+k_1x_{11}+k_2x_{12}+.....+k_nx_{1n}=y_1\\
k_0+k_1x_{21}+k_2x_{22}+.....+k_nx_{2n}=y_2\\
.\\
.\\
k_0+k_1x_{m1}+k_2x_{m2}+.....+k_nx_{mn}=y_m
$$
即：
$$
AK=Y
$$
对于最小二乘法来说，最终矩阵的表达形式为：
$$
min||AK-Y||^2_2\\
A属于R^{m^{(n+1)}},K属于R^{(n+1)},Y属于R^{m},其中m>=n，由于考虑到常数项，故属性值个数由n变为n+1
$$


推导过程可以用矩阵求导法则：向量积对列向量x求导运算法则

结果：
$$
(A^TA)K=A^TY  ---->K=(A^TA)^{-1}A^TY
$$

